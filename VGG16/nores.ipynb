{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T23:56:02.938279Z",
     "iopub.status.busy": "2025-07-26T23:56:02.937969Z",
     "iopub.status.idle": "2025-07-26T23:56:08.029008Z",
     "shell.execute_reply": "2025-07-26T23:56:08.028172Z",
     "shell.execute_reply.started": "2025-07-26T23:56:02.938242Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install tensorflowjs==2.8.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T23:56:08.031159Z",
     "iopub.status.busy": "2025-07-26T23:56:08.030864Z",
     "iopub.status.idle": "2025-07-26T23:56:14.394069Z",
     "shell.execute_reply": "2025-07-26T23:56:14.393112Z",
     "shell.execute_reply.started": "2025-07-26T23:56:08.031122Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflowjs as tfjs\n",
    "\n",
    "print(f\"Current TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Current TensorFlow JS version: {tfjs.__version__}\")\n",
    "print(f\"Keras version (via TensorFlow): {tf.keras.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-26T23:56:50.333348Z",
     "iopub.status.busy": "2025-07-26T23:56:50.333042Z",
     "iopub.status.idle": "2025-07-26T23:56:50.339975Z",
     "shell.execute_reply": "2025-07-26T23:56:50.339076Z",
     "shell.execute_reply.started": "2025-07-26T23:56:50.333299Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import shutil # For file operations\n",
    "from glob import glob \n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import cv2 # image processing\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing\n",
    "import matplotlib.pyplot as plt # data visualization\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define source paths for the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T23:56:51.638311Z",
     "iopub.status.busy": "2025-07-26T23:56:51.638039Z",
     "iopub.status.idle": "2025-07-26T23:56:51.644939Z",
     "shell.execute_reply": "2025-07-26T23:56:51.644219Z",
     "shell.execute_reply.started": "2025-07-26T23:56:51.638282Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create working directory structure\n",
    "output_base_dir = '/kaggle/working/balanced_xray_dataset'\n",
    "os.makedirs(output_base_dir, exist_ok=True)\n",
    "\n",
    "# Create temporary collection directories\n",
    "temp_collections = {\n",
    "    'tuberculosis': os.path.join(output_base_dir, 'temp_tuberculosis'),\n",
    "    'normal': os.path.join(output_base_dir, 'temp_normal'), \n",
    "    'pneumonia': os.path.join(output_base_dir, 'temp_pneumonia')\n",
    "}\n",
    "\n",
    "for path in temp_collections.values():\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Libraries imported and directories created!\")\n",
    "print(f\"üìÅ Working directory: {output_base_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T23:56:56.091232Z",
     "iopub.status.busy": "2025-07-26T23:56:56.090971Z",
     "iopub.status.idle": "2025-07-26T23:57:14.019902Z",
     "shell.execute_reply": "2025-07-26T23:57:14.019089Z",
     "shell.execute_reply.started": "2025-07-26T23:56:56.091207Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define all dataset paths and their structures\n",
    "dataset_configs = {\n",
    "    'tbx11k': {\n",
    "        'path': '/kaggle/input/tbx11k-simplified/tbx11k-simplified',\n",
    "        'type': 'csv_based',\n",
    "        'csv_file': 'data.csv',\n",
    "        'images_folder': 'images',\n",
    "        'mapping': {'tb': 'tuberculosis', 'no_tb': 'normal'}\n",
    "    },\n",
    "    'shenzhen': {\n",
    "        'path': '/kaggle/input/tuberculosis-chest-xrays-shenzhen',\n",
    "        'type': 'csv_based', \n",
    "        'csv_file': 'shenzhen_metadata.csv',\n",
    "        'images_folder': 'images/images',\n",
    "        'mapping': {'normal': 'normal', 'other': 'tuberculosis'}  # everything not normal is TB\n",
    "    },\n",
    "    'tb_chest_db': {\n",
    "        'path': '/kaggle/input/tuberculosis-tb-chest-xray-dataset/TB_Chest_Radiography_Database',\n",
    "        'type': 'folder_based',\n",
    "        'folders': {\n",
    "            'Normal': 'normal',\n",
    "            'Tuberculosis': 'tuberculosis'\n",
    "        }\n",
    "    },\n",
    "    'covid_pneumonia_tb': {\n",
    "        'path': '/kaggle/input/chest-xray-pneumoniacovid19tuberculosis',\n",
    "        'type': 'split_folder_based',\n",
    "        'splits': ['train', 'val', 'test'],\n",
    "        'folders': {\n",
    "            'NORMAL': 'normal',\n",
    "            'PNEUMONIA': 'pneumonia', \n",
    "            'TURBERCULOSIS': 'tuberculosis'\n",
    "        }\n",
    "    },\n",
    "    'chest_xray_pneumonia': {\n",
    "        'path': '/kaggle/input/chest-xray-pneumonia/chest_xray',\n",
    "        'type': 'split_folder_based',\n",
    "        'splits': ['train', 'val', 'test'],\n",
    "        'folders': {\n",
    "            'NORMAL': 'normal',\n",
    "            'PNEUMONIA': 'pneumonia'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def collect_images_from_datasets():\n",
    "    \"\"\"Collect all images from all datasets and organize by class\"\"\"\n",
    "    \n",
    "    collected_images = defaultdict(list)\n",
    "    \n",
    "    for dataset_name, config in dataset_configs.items():\n",
    "        print(f\"\\nüì• Processing {dataset_name}...\")\n",
    "        \n",
    "        if config['type'] == 'csv_based':\n",
    "            # Handle CSV-based datasets (TBX11K, Shenzhen)\n",
    "            csv_path = os.path.join(config['path'], config['csv_file'])\n",
    "            images_path = os.path.join(config['path'], config['images_folder'])\n",
    "            \n",
    "            if os.path.exists(csv_path):\n",
    "                df = pd.read_csv(csv_path)\n",
    "                \n",
    "                if dataset_name == 'tbx11k':\n",
    "                    # TBX11K specific handling\n",
    "                    for _, row in df.iterrows():\n",
    "                        target = row['target'].lower()\n",
    "                        if target in config['mapping']:\n",
    "                            class_name = config['mapping'][target]\n",
    "                            img_path = os.path.join(images_path, row['fname'])\n",
    "                            if os.path.exists(img_path):\n",
    "                                collected_images[class_name].append((img_path, dataset_name))\n",
    "                \n",
    "                elif dataset_name == 'shenzhen':\n",
    "                    # Shenzhen specific handling\n",
    "                    for _, row in df.iterrows():\n",
    "                        findings = row['findings'].lower().strip()\n",
    "                        if findings == 'normal':\n",
    "                            class_name = 'normal'\n",
    "                        else:\n",
    "                            class_name = 'tuberculosis'  # everything else is TB\n",
    "                        \n",
    "                        img_path = os.path.join(images_path, row['study_id'])\n",
    "                        if os.path.exists(img_path):\n",
    "                            collected_images[class_name].append((img_path, dataset_name))\n",
    "        \n",
    "        elif config['type'] == 'folder_based':\n",
    "            # Handle simple folder-based datasets (TB Chest DB)\n",
    "            for folder_name, class_name in config['folders'].items():\n",
    "                folder_path = os.path.join(config['path'], folder_name)\n",
    "                if os.path.exists(folder_path):\n",
    "                    for img_file in os.listdir(folder_path):\n",
    "                        if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                            img_path = os.path.join(folder_path, img_file)\n",
    "                            collected_images[class_name].append((img_path, dataset_name))\n",
    "        \n",
    "        elif config['type'] == 'split_folder_based':\n",
    "            # Handle pre-split datasets (COVID+Pneumonia+TB, Chest X-ray Pneumonia)\n",
    "            for split in config['splits']:\n",
    "                split_path = os.path.join(config['path'], split)\n",
    "                if os.path.exists(split_path):\n",
    "                    for folder_name, class_name in config['folders'].items():\n",
    "                        folder_path = os.path.join(split_path, folder_name)\n",
    "                        if os.path.exists(folder_path):\n",
    "                            for img_file in os.listdir(folder_path):\n",
    "                                if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                                    img_path = os.path.join(folder_path, img_file)\n",
    "                                    collected_images[class_name].append((img_path, f\"{dataset_name}_{split}\"))\n",
    "        \n",
    "        # Print progress for this dataset\n",
    "        dataset_counts = defaultdict(int)\n",
    "        for class_name, images in collected_images.items():\n",
    "            dataset_counts[class_name] += len([img for img in images if img[1].startswith(dataset_name)])\n",
    "        \n",
    "        for class_name, count in dataset_counts.items():\n",
    "            if count > 0:\n",
    "                print(f\"  {class_name}: {count} images\")\n",
    "    \n",
    "    return collected_images\n",
    "\n",
    "# Collect all images\n",
    "print(\"üîç Scanning all datasets for images...\")\n",
    "all_collected_images = collect_images_from_datasets()\n",
    "\n",
    "# Print final collection summary\n",
    "print(f\"\\nüìä Total Images Collected:\")\n",
    "for class_name, images in all_collected_images.items():\n",
    "    print(f\"  {class_name.title()}: {len(images)} images\")\n",
    "    \n",
    "    # Show distribution by dataset\n",
    "    dataset_dist = defaultdict(int)\n",
    "    for _, dataset in images:\n",
    "        dataset_dist[dataset] += 1\n",
    "    \n",
    "    for dataset, count in sorted(dataset_dist.items()):\n",
    "        print(f\"    ‚îî‚îÄ {dataset}: {count}\")\n",
    "\n",
    "print(f\"\\n‚úÖ All datasets scanned and catalogued!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T23:57:18.120087Z",
     "iopub.status.busy": "2025-07-26T23:57:18.119844Z",
     "iopub.status.idle": "2025-07-26T23:57:48.832847Z",
     "shell.execute_reply": "2025-07-26T23:57:48.832131Z",
     "shell.execute_reply.started": "2025-07-26T23:57:18.120065Z"
    }
   },
   "outputs": [],
   "source": [
    "# Copy all tuberculosis images to temporary collection folder\n",
    "print(\"ü¶† Copying all Tuberculosis images...\")\n",
    "\n",
    "tuberculosis_images = all_collected_images.get('tuberculosis', [])\n",
    "tuberculosis_dest = temp_collections['tuberculosis']\n",
    "\n",
    "# Copy all available tuberculosis images\n",
    "copied_count = 0\n",
    "failed_count = 0\n",
    "\n",
    "for img_path, dataset_source in tqdm(tuberculosis_images, desc=\"Copying TB images\"):\n",
    "    try:\n",
    "        # Create unique filename to avoid conflicts\n",
    "        base_name = os.path.basename(img_path)\n",
    "        unique_filename = f\"{dataset_source}_{base_name}\"\n",
    "        dest_path = os.path.join(tuberculosis_dest, unique_filename)\n",
    "        \n",
    "        # Copy the file\n",
    "        shutil.copy2(img_path, dest_path)\n",
    "        copied_count += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to copy {img_path}: {e}\")\n",
    "        failed_count += 1\n",
    "\n",
    "print(f\"\\nüìã Tuberculosis Images Summary:\")\n",
    "print(f\"  ‚úÖ Successfully copied: {copied_count}\")\n",
    "print(f\"  ‚ùå Failed to copy: {failed_count}\")\n",
    "print(f\"  üìÅ Total in collection: {len(os.listdir(tuberculosis_dest))}\")\n",
    "\n",
    "# Verify we have the expected count\n",
    "actual_tb_count = len(os.listdir(tuberculosis_dest))\n",
    "if actual_tb_count < 2538:\n",
    "    print(f\"‚ö†Ô∏è  Warning: Only found {actual_tb_count} TB images, less than target 2538\")\n",
    "else:\n",
    "    print(f\"‚úÖ Great! We have {actual_tb_count} TB images (target was 2538)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T23:57:58.037053Z",
     "iopub.status.busy": "2025-07-26T23:57:58.036764Z",
     "iopub.status.idle": "2025-07-26T23:58:27.499259Z",
     "shell.execute_reply": "2025-07-26T23:58:27.498537Z",
     "shell.execute_reply.started": "2025-07-26T23:57:58.037027Z"
    }
   },
   "outputs": [],
   "source": [
    "# Copy 2538 normal images with smart distribution\n",
    "print(\"‚úÖ Copying Normal images with balanced distribution...\")\n",
    "\n",
    "normal_images = all_collected_images.get('normal', [])\n",
    "normal_dest = temp_collections['normal']\n",
    "\n",
    "# Analyze available normal images by dataset quality/preference\n",
    "dataset_preferences = {\n",
    "    # High quality datasets (prefer these)\n",
    "    'chest_xray_pneumonia_train': 0.4,  # High quality, large dataset\n",
    "    'chest_xray_pneumonia_val': 0.05,\n",
    "    'chest_xray_pneumonia_test': 0.05,\n",
    "    'tbx11k': 0.3,  # Good quality TB-focused dataset\n",
    "    'shenzhen': 0.1,  # Smaller but good quality\n",
    "    'tb_chest_db': 0.05,  # Smaller contribution\n",
    "    'covid_pneumonia_tb_train': 0.03,\n",
    "    'covid_pneumonia_tb_val': 0.01,\n",
    "    'covid_pneumonia_tb_test': 0.01\n",
    "}\n",
    "\n",
    "# Group images by dataset\n",
    "normal_by_dataset = defaultdict(list)\n",
    "for img_path, dataset_source in normal_images:\n",
    "    normal_by_dataset[dataset_source].append(img_path)\n",
    "\n",
    "print(f\"üìä Available Normal Images by Dataset:\")\n",
    "for dataset, images in normal_by_dataset.items():\n",
    "    print(f\"  {dataset}: {len(images)} images\")\n",
    "\n",
    "# Calculate target count per dataset based on preferences and availability\n",
    "target_count = 2538\n",
    "selected_images = []\n",
    "\n",
    "print(f\"\\nüìã Selecting {target_count} normal images with smart distribution:\")\n",
    "\n",
    "for dataset, preference in dataset_preferences.items():\n",
    "    if dataset in normal_by_dataset:\n",
    "        available = len(normal_by_dataset[dataset])\n",
    "        target_from_this = int(target_count * preference)\n",
    "        actual_from_this = min(target_from_this, available)\n",
    "        \n",
    "        # Randomly sample from this dataset\n",
    "        random.shuffle(normal_by_dataset[dataset])\n",
    "        selected_from_this = normal_by_dataset[dataset][:actual_from_this]\n",
    "        \n",
    "        for img_path in selected_from_this:\n",
    "            selected_images.append((img_path, dataset))\n",
    "        \n",
    "        print(f\"  {dataset}: {actual_from_this}/{available} images (target: {target_from_this})\")\n",
    "\n",
    "# If we don't have enough, fill from remaining datasets\n",
    "remaining_needed = target_count - len(selected_images)\n",
    "if remaining_needed > 0:\n",
    "    print(f\"\\nüîÑ Need {remaining_needed} more images, filling from remaining...\")\n",
    "    \n",
    "    # Collect unused images\n",
    "    used_paths = {img[0] for img in selected_images}\n",
    "    unused_images = [(path, dataset) for dataset, paths in normal_by_dataset.items() \n",
    "                    for path in paths if path not in used_paths]\n",
    "    \n",
    "    random.shuffle(unused_images)\n",
    "    additional_images = unused_images[:remaining_needed]\n",
    "    selected_images.extend(additional_images)\n",
    "    \n",
    "    print(f\"  Added {len(additional_images)} additional images\")\n",
    "\n",
    "# Now copy the selected images\n",
    "print(f\"\\nüì• Copying {len(selected_images)} selected normal images...\")\n",
    "\n",
    "copied_count = 0\n",
    "failed_count = 0\n",
    "\n",
    "for img_path, dataset_source in tqdm(selected_images, desc=\"Copying normal images\"):\n",
    "    try:\n",
    "        # Create unique filename\n",
    "        base_name = os.path.basename(img_path)\n",
    "        unique_filename = f\"{dataset_source}_{base_name}\"\n",
    "        dest_path = os.path.join(normal_dest, unique_filename)\n",
    "        \n",
    "        # Copy the file\n",
    "        shutil.copy2(img_path, dest_path)\n",
    "        copied_count += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to copy {img_path}: {e}\")\n",
    "        failed_count += 1\n",
    "\n",
    "print(f\"\\nüìã Normal Images Summary:\")\n",
    "print(f\"  ‚úÖ Successfully copied: {copied_count}\")\n",
    "print(f\"  ‚ùå Failed to copy: {failed_count}\")\n",
    "print(f\"  üìÅ Total in collection: {len(os.listdir(normal_dest))}\")\n",
    "print(f\"  üéØ Target was: {target_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T23:58:28.986162Z",
     "iopub.status.busy": "2025-07-26T23:58:28.985911Z",
     "iopub.status.idle": "2025-07-26T23:58:45.840869Z",
     "shell.execute_reply": "2025-07-26T23:58:45.840173Z",
     "shell.execute_reply.started": "2025-07-26T23:58:28.986138Z"
    }
   },
   "outputs": [],
   "source": [
    "# Copy 2538 pneumonia images with smart distribution\n",
    "print(\"ü´Å Copying Pneumonia images with balanced distribution...\")\n",
    "\n",
    "pneumonia_images = all_collected_images.get('pneumonia', [])\n",
    "pneumonia_dest = temp_collections['pneumonia']\n",
    "\n",
    "# Analyze available pneumonia images by dataset\n",
    "pneumonia_by_dataset = defaultdict(list)\n",
    "for img_path, dataset_source in pneumonia_images:\n",
    "    pneumonia_by_dataset[dataset_source].append(img_path)\n",
    "\n",
    "print(f\"üìä Available Pneumonia Images by Dataset:\")\n",
    "total_available = 0\n",
    "for dataset, images in pneumonia_by_dataset.items():\n",
    "    print(f\"  {dataset}: {len(images)} images\")\n",
    "    total_available += len(images)\n",
    "\n",
    "print(f\"  üìä Total Available: {total_available} images\")\n",
    "\n",
    "# Dataset preferences for pneumonia (prioritize high-quality datasets)\n",
    "pneumonia_preferences = {\n",
    "    'chest_xray_pneumonia_train': 0.85,  # Main high-quality pneumonia dataset\n",
    "    'chest_xray_pneumonia_val': 0.05,\n",
    "    'chest_xray_pneumonia_test': 0.05,\n",
    "    'covid_pneumonia_tb_train': 0.03,\n",
    "    'covid_pneumonia_tb_val': 0.01,\n",
    "    'covid_pneumonia_tb_test': 0.01\n",
    "}\n",
    "\n",
    "# Calculate target count per dataset\n",
    "target_count = 2538\n",
    "selected_images = []\n",
    "\n",
    "print(f\"\\nüìã Selecting {target_count} pneumonia images with smart distribution:\")\n",
    "\n",
    "for dataset, preference in pneumonia_preferences.items():\n",
    "    if dataset in pneumonia_by_dataset:\n",
    "        available = len(pneumonia_by_dataset[dataset])\n",
    "        target_from_this = int(target_count * preference)\n",
    "        actual_from_this = min(target_from_this, available)\n",
    "        \n",
    "        # Randomly sample from this dataset\n",
    "        random.shuffle(pneumonia_by_dataset[dataset])\n",
    "        selected_from_this = pneumonia_by_dataset[dataset][:actual_from_this]\n",
    "        \n",
    "        for img_path in selected_from_this:\n",
    "            selected_images.append((img_path, dataset))\n",
    "        \n",
    "        print(f\"  {dataset}: {actual_from_this}/{available} images (target: {target_from_this})\")\n",
    "\n",
    "# Fill remaining if needed\n",
    "remaining_needed = target_count - len(selected_images)\n",
    "if remaining_needed > 0:\n",
    "    print(f\"\\nüîÑ Need {remaining_needed} more images, filling from remaining...\")\n",
    "    \n",
    "    # Collect unused images\n",
    "    used_paths = {img[0] for img in selected_images}\n",
    "    unused_images = [(path, dataset) for dataset, paths in pneumonia_by_dataset.items() \n",
    "                    for path in paths if path not in used_paths]\n",
    "    \n",
    "    random.shuffle(unused_images)\n",
    "    additional_images = unused_images[:remaining_needed]\n",
    "    selected_images.extend(additional_images)\n",
    "    \n",
    "    print(f\"  Added {len(additional_images)} additional images\")\n",
    "\n",
    "# Trim if we have too many\n",
    "if len(selected_images) > target_count:\n",
    "    print(f\"üîÑ Trimming {len(selected_images) - target_count} excess images...\")\n",
    "    random.shuffle(selected_images)\n",
    "    selected_images = selected_images[:target_count]\n",
    "\n",
    "# Copy the selected images\n",
    "print(f\"\\nüì• Copying {len(selected_images)} selected pneumonia images...\")\n",
    "\n",
    "copied_count = 0\n",
    "failed_count = 0\n",
    "\n",
    "for img_path, dataset_source in tqdm(selected_images, desc=\"Copying pneumonia images\"):\n",
    "    try:\n",
    "        # Create unique filename\n",
    "        base_name = os.path.basename(img_path)\n",
    "        unique_filename = f\"{dataset_source}_{base_name}\"\n",
    "        dest_path = os.path.join(pneumonia_dest, unique_filename)\n",
    "        \n",
    "        # Copy the file\n",
    "        shutil.copy2(img_path, dest_path)\n",
    "        copied_count += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to copy {img_path}: {e}\")\n",
    "        failed_count += 1\n",
    "\n",
    "print(f\"\\nüìã Pneumonia Images Summary:\")\n",
    "print(f\"  ‚úÖ Successfully copied: {copied_count}\")\n",
    "print(f\"  ‚ùå Failed to copy: {failed_count}\")\n",
    "print(f\"  üìÅ Total in collection: {len(os.listdir(pneumonia_dest))}\")\n",
    "print(f\"  üéØ Target was: {target_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T23:58:46.776688Z",
     "iopub.status.busy": "2025-07-26T23:58:46.776431Z",
     "iopub.status.idle": "2025-07-26T23:58:55.259292Z",
     "shell.execute_reply": "2025-07-26T23:58:55.258478Z",
     "shell.execute_reply.started": "2025-07-26T23:58:46.776666Z"
    }
   },
   "outputs": [],
   "source": [
    "# Distribute collected images into train/val/test splits\n",
    "print(\"üìä Creating balanced train/val/test splits...\")\n",
    "\n",
    "# Define split ratios\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15  \n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "# Create final directory structure\n",
    "final_data_dir = os.path.join(output_base_dir, 'final')\n",
    "categories = ['Normal', 'Tuberculosis', 'Pneumonia']\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    for category in categories:\n",
    "        os.makedirs(os.path.join(final_data_dir, split, category), exist_ok=True)\n",
    "\n",
    "def split_and_copy_images(source_dir, category_name, target_base_dir):\n",
    "    \"\"\"Split images from source directory into train/val/test\"\"\"\n",
    "    \n",
    "    # Get all images from source\n",
    "    all_images = [f for f in os.listdir(source_dir) \n",
    "                  if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    # Shuffle for random distribution\n",
    "    random.shuffle(all_images)\n",
    "    \n",
    "    # Calculate split sizes\n",
    "    total = len(all_images)\n",
    "    train_size = int(total * TRAIN_RATIO)\n",
    "    val_size = int(total * VAL_RATIO)\n",
    "    test_size = total - train_size - val_size  # Remaining goes to test\n",
    "    \n",
    "    # Split the images\n",
    "    train_images = all_images[:train_size]\n",
    "    val_images = all_images[train_size:train_size + val_size]\n",
    "    test_images = all_images[train_size + val_size:]\n",
    "    \n",
    "    print(f\"\\nüìÅ {category_name} Distribution:\")\n",
    "    print(f\"  Train: {len(train_images)} images ({len(train_images)/total*100:.1f}%)\")\n",
    "    print(f\"  Val:   {len(val_images)} images ({len(val_images)/total*100:.1f}%)\")\n",
    "    print(f\"  Test:  {len(test_images)} images ({len(test_images)/total*100:.1f}%)\")\n",
    "    \n",
    "    # Copy images to respective directories\n",
    "    splits = {\n",
    "        'train': train_images,\n",
    "        'val': val_images, \n",
    "        'test': test_images\n",
    "    }\n",
    "    \n",
    "    copied_counts = {'train': 0, 'val': 0, 'test': 0}\n",
    "    \n",
    "    for split_name, image_list in splits.items():\n",
    "        dest_dir = os.path.join(target_base_dir, split_name, category_name)\n",
    "        \n",
    "        for img_file in tqdm(image_list, desc=f\"Copying {category_name} to {split_name}\"):\n",
    "            try:\n",
    "                src_path = os.path.join(source_dir, img_file)\n",
    "                dest_path = os.path.join(dest_dir, img_file)\n",
    "                shutil.copy2(src_path, dest_path)\n",
    "                copied_counts[split_name] += 1\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to copy {img_file}: {e}\")\n",
    "    \n",
    "    return copied_counts\n",
    "\n",
    "# Process each category\n",
    "total_stats = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Process Tuberculosis\n",
    "print(\"ü¶† Processing Tuberculosis images...\")\n",
    "tb_stats = split_and_copy_images(\n",
    "    temp_collections['tuberculosis'], \n",
    "    'Tuberculosis', \n",
    "    final_data_dir\n",
    ")\n",
    "for split, count in tb_stats.items():\n",
    "    total_stats[split]['Tuberculosis'] = count\n",
    "\n",
    "# Process Normal\n",
    "print(\"\\n‚úÖ Processing Normal images...\")\n",
    "normal_stats = split_and_copy_images(\n",
    "    temp_collections['normal'], \n",
    "    'Normal', \n",
    "    final_data_dir\n",
    ")\n",
    "for split, count in normal_stats.items():\n",
    "    total_stats[split]['Normal'] = count\n",
    "\n",
    "# Process Pneumonia\n",
    "print(\"\\nü´Å Processing Pneumonia images...\")\n",
    "pneumonia_stats = split_and_copy_images(\n",
    "    temp_collections['pneumonia'], \n",
    "    'Pneumonia', \n",
    "    final_data_dir\n",
    ")\n",
    "for split, count in pneumonia_stats.items():\n",
    "    total_stats[split]['Pneumonia'] = count\n",
    "\n",
    "# Clean up temporary directories\n",
    "print(\"\\nüßπ Cleaning up temporary directories...\")\n",
    "for temp_dir in temp_collections.values():\n",
    "    shutil.rmtree(temp_dir)\n",
    "\n",
    "# Final summary\n",
    "print(\"\\nüéâ FINAL BALANCED DATASET SUMMARY:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    print(f\"\\n{split.upper()}:\")\n",
    "    split_total = 0\n",
    "    for category in categories:\n",
    "        count = total_stats[split][category]\n",
    "        split_total += count\n",
    "        print(f\"  {category:12}: {count:4d} images\")\n",
    "    print(f\"  {'TOTAL':12}: {split_total:4d} images\")\n",
    "\n",
    "# Verify balance\n",
    "print(f\"\\nüìä Class Balance Check:\")\n",
    "for category in categories:\n",
    "    category_total = sum(total_stats[split][category] for split in ['train', 'val', 'test'])\n",
    "    print(f\"  {category}: {category_total} total images\")\n",
    "\n",
    "print(f\"\\n‚úÖ Balanced dataset created successfully!\")\n",
    "print(f\"üìÅ Final dataset location: {final_data_dir}\")\n",
    "\n",
    "# Update paths for model training\n",
    "train_dir = os.path.join(final_data_dir, 'train')\n",
    "val_dir = os.path.join(final_data_dir, 'val') \n",
    "test_dir = os.path.join(final_data_dir, 'test')\n",
    "\n",
    "print(f\"\\nüìç Directory paths for model training:\")\n",
    "print(f\"  Train: {train_dir}\")\n",
    "print(f\"  Val:   {val_dir}\")\n",
    "print(f\"  Test:  {test_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T23:59:14.244371Z",
     "iopub.status.busy": "2025-07-26T23:59:14.244058Z",
     "iopub.status.idle": "2025-07-26T23:59:14.987049Z",
     "shell.execute_reply": "2025-07-26T23:59:14.986208Z",
     "shell.execute_reply.started": "2025-07-26T23:59:14.244328Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(18, 6)) # 1 row, 3 columns for 3 classes\n",
    "ax = ax.ravel()\n",
    "plt.tight_layout()\n",
    "\n",
    "for i, category in enumerate(categories):\n",
    "    set_path = train_dir \n",
    "    try:\n",
    "        img_file = os.path.join(set_path, category, os.listdir(os.path.join(set_path, category))[0])\n",
    "        ax[i].imshow(plt.imread(img_file))\n",
    "        ax[i].set_title(f'Train Set - Class: {category}')\n",
    "        ax[i].axis('off')\n",
    "    except IndexError:\n",
    "        ax[i].set_title(f'Train Set - No images for {category}')\n",
    "        ax[i].axis('off')\n",
    "    except FileNotFoundError:\n",
    "        ax[i].set_title(f'Train Set - Path not found for {category}')\n",
    "        ax[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T23:59:27.103055Z",
     "iopub.status.busy": "2025-07-26T23:59:27.102805Z",
     "iopub.status.idle": "2025-07-26T23:59:27.533300Z",
     "shell.execute_reply": "2025-07-26T23:59:27.532611Z",
     "shell.execute_reply.started": "2025-07-26T23:59:27.103032Z"
    }
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 150\n",
    "BATCH_SIZE = 32 # You can adjust this\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='sparse',  # ‚úÖ Use 'sparse'\n",
    "    classes=sorted(categories),\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator = val_test_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='sparse',  # ‚úÖ Use 'sparse'\n",
    "    classes=sorted(categories),\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='sparse',  # ‚úÖ Use 'sparse'\n",
    "    classes=sorted(categories),\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "# Verify class indices\n",
    "print(\"Class Indices:\", train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T23:59:31.827515Z",
     "iopub.status.busy": "2025-07-26T23:59:31.827196Z",
     "iopub.status.idle": "2025-07-26T23:59:36.067122Z",
     "shell.execute_reply": "2025-07-26T23:59:36.066482Z",
     "shell.execute_reply.started": "2025-07-26T23:59:31.827486Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_multilabel_model(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), num_classes=3, lr=1e-4):\n",
    "    \"\"\"\n",
    "    Build a multi-label classification model using VGG16 backbone\n",
    "    - Uses sigmoid activation (not softmax)\n",
    "    - Each class gets independent probability\n",
    "    - Probabilities DON'T need to sum to 100%\n",
    "    \"\"\"\n",
    "    # Load VGG16 base model\n",
    "    base_vgg16 = VGG16(\n",
    "        weights='imagenet',\n",
    "        include_top=False, \n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    \n",
    "    model = Sequential([\n",
    "        base_vgg16,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='sigmoid')  \n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Instantiate the NEW model\n",
    "num_classes = len(categories)\n",
    "model = build_multilabel_model(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), num_classes=num_classes, lr=1e-4)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T23:59:44.377832Z",
     "iopub.status.busy": "2025-07-26T23:59:44.377477Z",
     "iopub.status.idle": "2025-07-26T23:59:44.390825Z",
     "shell.execute_reply": "2025-07-26T23:59:44.390096Z",
     "shell.execute_reply.started": "2025-07-26T23:59:44.377793Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class MultiLabelGenerator(Sequence):\n",
    "    \"\"\"\n",
    "    Custom generator that converts single-class labels to multi-label format\n",
    "    \"\"\"\n",
    "    def __init__(self, base_generator, class_indices, multi_label_rules=None):\n",
    "        self.base_generator = base_generator\n",
    "        self.class_indices = class_indices\n",
    "        self.num_classes = len(class_indices)\n",
    "        \n",
    "        # Default rules: if no multi-label rules provided, use single-label mapping\n",
    "        self.multi_label_rules = multi_label_rules or self._create_default_rules()\n",
    "        \n",
    "    def _create_default_rules(self):\n",
    "        \"\"\"\n",
    "        Create default single-label to multi-label mapping\n",
    "        You can customize this based on medical knowledge\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'Normal': [1.0, 0.0, 0.0],      # 100% Normal, 0% others\n",
    "            'Pneumonia': [0.1, 1.0, 0.2],  # 10% Normal, 100% Pneumonia, 20% TB (co-occurrence)  \n",
    "            'Tuberculosis': [0.1, 0.3, 1.0] # 10% Normal, 30% Pneumonia, 100% TB (co-occurrence)\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.base_generator)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Get batch from base generator\n",
    "        batch_x, batch_y_single = self.base_generator[index]\n",
    "        batch_size = len(batch_x)\n",
    "        \n",
    "        # Convert single labels to multi-label format\n",
    "        batch_y_multi = np.zeros((batch_size, self.num_classes))\n",
    "        \n",
    "        for i, class_index in enumerate(batch_y_single):\n",
    "            class_index = int(class_index)\n",
    "            # Get class name from index\n",
    "            class_name = [k for k, v in self.class_indices.items() if v == class_index][0]\n",
    "            # Apply multi-label rule\n",
    "            batch_y_multi[i] = self.multi_label_rules[class_name]\n",
    "        \n",
    "        return batch_x, batch_y_multi\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.base_generator.on_epoch_end()\n",
    "\n",
    "# Create multi-label generators\n",
    "train_ml_generator = MultiLabelGenerator(\n",
    "    train_generator, \n",
    "    train_generator.class_indices,\n",
    "    multi_label_rules={\n",
    "        'Normal': [0.9, 0.05, 0.05],     # Mostly normal, slight chance of others\n",
    "        'Pneumonia': [0.1, 0.9, 0.3],   # Strong pneumonia, some TB co-occurrence\n",
    "        'Tuberculosis': [0.1, 0.4, 0.9] # Strong TB, some pneumonia co-occurrence\n",
    "    }\n",
    ")\n",
    "\n",
    "val_ml_generator = MultiLabelGenerator(validation_generator, validation_generator.class_indices)\n",
    "test_ml_generator = MultiLabelGenerator(test_generator, test_generator.class_indices)\n",
    "\n",
    "print(\"Multi-label generators created!\")\n",
    "print(\"Training samples:\", len(train_ml_generator))\n",
    "print(\"Validation samples:\", len(val_ml_generator)) \n",
    "print(\"Test samples:\", len(test_ml_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T00:13:33.541098Z",
     "iopub.status.busy": "2025-07-27T00:13:33.540839Z",
     "iopub.status.idle": "2025-07-27T00:13:33.683238Z",
     "shell.execute_reply": "2025-07-27T00:13:33.682333Z",
     "shell.execute_reply.started": "2025-07-27T00:13:33.541073Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Custom loss to discourage Normal + Disease predictions\n",
    "def constrained_binary_crossentropy(y_true, y_pred):\n",
    "    # Standard binary crossentropy\n",
    "    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "    # Extract predicted probabilities\n",
    "    normal = y_pred[:, 0]        # Assuming class order: [Normal, Pneumonia, Tuberculosis]\n",
    "    pneumonia = y_pred[:, 1]\n",
    "    tb = y_pred[:, 2]\n",
    "\n",
    "    # Penalize when Normal and any disease are predicted together\n",
    "    disease = tf.maximum(pneumonia, tb)\n",
    "    conflict_penalty = normal * disease  # High if both Normal and Disease predicted\n",
    "\n",
    "    # Return combined loss (0.3 = penalty weight, tune as needed)\n",
    "    return bce + 0.3 * conflict_penalty\n",
    "\n",
    "# Compile with custom loss\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=constrained_binary_crossentropy,\n",
    "    metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]\n",
    ")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Model compiled with custom constrained loss to reduce Normal+Disease errors.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T23:59:50.697645Z",
     "iopub.status.busy": "2025-07-26T23:59:50.697383Z",
     "iopub.status.idle": "2025-07-27T00:04:11.306459Z",
     "shell.execute_reply": "2025-07-27T00:04:11.305602Z",
     "shell.execute_reply.started": "2025-07-26T23:59:50.697622Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define early stopping\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',       # or 'val_accuracy', depending on your goal\n",
    "    patience=5,               # stop after 5 epochs with no improvement\n",
    "    restore_best_weights=True # rollback to the best model\n",
    ")\n",
    "\n",
    "# Increase epochs (won‚Äôt always run all of them if early stopping triggers)\n",
    "EPOCHS = 50\n",
    "\n",
    "# Train with early stopping\n",
    "history = model.fit(\n",
    "    train_ml_generator,\n",
    "    steps_per_epoch=len(train_ml_generator),\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ml_generator,\n",
    "    validation_steps=len(val_ml_generator),\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Multi-label training completed with early stopping!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T00:05:11.470682Z",
     "iopub.status.busy": "2025-07-27T00:05:11.470395Z",
     "iopub.status.idle": "2025-07-27T00:06:09.203632Z",
     "shell.execute_reply": "2025-07-27T00:06:09.202819Z",
     "shell.execute_reply.started": "2025-07-27T00:05:11.470654Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "\n",
    "print(\"\\nEvaluating Multi-Label Model on Test Set...\")\n",
    "test_loss, test_accuracy, test_precision, test_recall  = model.evaluate(test_ml_generator, steps=len(test_ml_generator))\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "\n",
    "# Get predictions\n",
    "Y_pred = model.predict(test_ml_generator, steps=len(test_ml_generator))\n",
    "print(f\"Prediction shape: {Y_pred.shape}\")\n",
    "\n",
    "# Get true labels\n",
    "Y_true = []\n",
    "for i in range(len(test_ml_generator)):\n",
    "    _, y_batch = test_ml_generator[i]\n",
    "    Y_true.extend(y_batch)\n",
    "Y_true = np.array(Y_true)\n",
    "print(f\"True labels shape: {Y_true.shape}\")\n",
    "\n",
    "# Convert both Y_true and Y_pred to binary (threshold = 0.5)\n",
    "Y_true_binary = (Y_true > 0.5).astype(int)\n",
    "Y_pred_binary = (Y_pred > 0.5).astype(int)\n",
    "\n",
    "# Multi-label evaluation\n",
    "print(\"\\n=== MULTI-LABEL EVALUATION ===\")\n",
    "class_names = ['Normal', 'Pneumonia', 'Tuberculosis']\n",
    "\n",
    "# Per-class evaluation\n",
    "for i, class_name in enumerate(class_names):\n",
    "    print(f\"\\n{class_name}:\")\n",
    "    print(f\"  True positives: {np.sum((Y_true_binary[:, i] == 1) & (Y_pred_binary[:, i] == 1))}\")\n",
    "    print(f\"  False positives: {np.sum((Y_true_binary[:, i] == 0) & (Y_pred_binary[:, i] == 1))}\")\n",
    "    print(f\"  False negatives: {np.sum((Y_true_binary[:, i] == 1) & (Y_pred_binary[:, i] == 0))}\")\n",
    "    print(f\"  Average probability: {np.mean(Y_pred[:, i]):.3f}\")\n",
    "\n",
    "# Confusion matrices for each class\n",
    "confusion_matrices = multilabel_confusion_matrix(Y_true_binary, Y_pred_binary)\n",
    "for i, (class_name, cm) in enumerate(zip(class_names, confusion_matrices)):\n",
    "    print(f\"\\nConfusion Matrix for {class_name}:\")\n",
    "    print(cm)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(Y_true_binary, Y_pred_binary, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T00:06:34.953156Z",
     "iopub.status.busy": "2025-07-27T00:06:34.952864Z",
     "iopub.status.idle": "2025-07-27T00:06:35.371465Z",
     "shell.execute_reply": "2025-07-27T00:06:35.370728Z",
     "shell.execute_reply.started": "2025-07-27T00:06:34.953131Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_multilabel_diseases(model, img_path, confidence_threshold=0.5, image_size=150):\n",
    "    \"\"\"\n",
    "    Multi-label prediction function - can detect multiple diseases simultaneously\n",
    "    \"\"\"\n",
    "    # Preprocess image\n",
    "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(image_size, image_size))\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = img_array / 255.0\n",
    "    \n",
    "    # Make prediction - now returns independent probabilities\n",
    "    predictions = model.predict(img_array, verbose=0)[0]\n",
    "    \n",
    "    # Class names\n",
    "    class_names = ['Normal', 'Pneumonia', 'Tuberculosis']\n",
    "    \n",
    "    print(\"=== MULTI-LABEL PREDICTIONS ===\")\n",
    "    print(f\"Image: {img_path}\")\n",
    "    print(\"\\nIndependent Probabilities:\")\n",
    "    \n",
    "    detections = {}\n",
    "    for i, (name, prob) in enumerate(zip(class_names, predictions)):\n",
    "        is_detected = prob > confidence_threshold\n",
    "        print(f\"  {name}: {prob:.1%} {'‚úì DETECTED' if is_detected else ''}\")\n",
    "        \n",
    "        detections[name.lower()] = {\n",
    "            'detected': is_detected,\n",
    "            'confidence': float(prob)\n",
    "        }\n",
    "    \n",
    "    # Show what's detected\n",
    "    detected_diseases = [name for name, data in detections.items() if data['detected']]\n",
    "    \n",
    "    if len(detected_diseases) == 0:\n",
    "        print(f\"\\nüü¢ NO DISEASES DETECTED (all below {confidence_threshold:.0%} threshold)\")\n",
    "    elif len(detected_diseases) == 1:\n",
    "        disease = detected_diseases[0].title()\n",
    "        conf = detections[detected_diseases[0]]['confidence']\n",
    "        print(f\"\\nüîµ SINGLE DISEASE: {disease} ({conf:.1%})\")\n",
    "    else:\n",
    "        diseases_str = \", \".join([d.title() for d in detected_diseases])\n",
    "        print(f\"\\nüî¥ MULTIPLE DISEASES DETECTED: {diseases_str}\")\n",
    "        print(\"This patient may have co-occurring conditions!\")\n",
    "    \n",
    "    return detections\n",
    "\n",
    "# Test the multi-label prediction\n",
    "def test_multilabel_predictions():\n",
    "    \"\"\"Test multi-label predictions on sample images\"\"\"\n",
    "    \n",
    "    # Test on each class\n",
    "    for class_name in ['Normal', 'Tuberculosis', 'Pneumonia']:\n",
    "        class_dir = os.path.join(test_dir, class_name)\n",
    "        if os.path.exists(class_dir):\n",
    "            image_files = [f for f in os.listdir(class_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "            if image_files:\n",
    "                test_image = os.path.join(class_dir, image_files[0])\n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(f\"TESTING: {class_name} sample\")\n",
    "                print('='*60)\n",
    "                results = predict_multilabel_diseases(model, test_image, confidence_threshold=0.3)\n",
    "                \n",
    "# Run the test\n",
    "test_multilabel_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import tensorflowjs as tfjs\n",
    "\n",
    "# === SAVE WEIGHTS ===\n",
    "weights_filename = \"vgg16_multilabel_chest_xray.weights.h5\"\n",
    "model.save_weights(weights_filename)\n",
    "print(f\"‚úÖ Weights saved as: {weights_filename}\")\n",
    "\n",
    "# === EXPORT TO TFJS FORMAT ===\n",
    "tfjs_output_dir = \"tfjs_multilabel_model\"\n",
    "os.makedirs(tfjs_output_dir, exist_ok=True)\n",
    "\n",
    "# Convert to TFJS format (from in-memory Keras model)\n",
    "tfjs.converters.save_keras_model(model, tfjs_output_dir)\n",
    "print(f\"\\n‚úÖ TFJS model saved to: {tfjs_output_dir}/\")\n",
    "\n",
    "# === WRITE METADATA FILE ===\n",
    "metadata = {\n",
    "    \"model_info\": {\n",
    "        \"type\": \"multi_label\",\n",
    "        \"input_shape\": [IMAGE_SIZE, IMAGE_SIZE, 3],\n",
    "        \"num_classes\": 3,\n",
    "        \"class_names\": [\"Normal\", \"Pneumonia\", \"Tuberculosis\"],\n",
    "        \"activation\": \"sigmoid\",\n",
    "        \"confidence_threshold\": 0.5,\n",
    "        \"independent_predictions\": True\n",
    "    },\n",
    "    \"disease_config\": {\n",
    "        \"normal\": {\"class_index\": 0, \"color\": \"#00FF00\", \"threshold\": 0.5},\n",
    "        \"pneumonia\": {\"class_index\": 1, \"color\": \"#FFA500\", \"threshold\": 0.5},\n",
    "        \"tuberculosis\": {\"class_index\": 2, \"color\": \"#FF0000\", \"threshold\": 0.5}\n",
    "    },\n",
    "    \"preprocessing\": {\n",
    "        \"rescale\": \"1./255\",\n",
    "        \"target_size\": [IMAGE_SIZE, IMAGE_SIZE]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metadata JSON\n",
    "metadata_path = os.path.join(tfjs_output_dir, 'multilabel_metadata.json')\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(\"‚úÖ Metadata JSON saved!\")\n",
    "\n",
    "# === ZIP EVERYTHING ===\n",
    "zip_filename = \"multilabel_chest_xray_tfjs.zip\"\n",
    "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for root, _, files in os.walk(tfjs_output_dir):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            arcname = os.path.relpath(file_path, tfjs_output_dir)\n",
    "            zipf.write(file_path, arcname)\n",
    "\n",
    "print(f\"\\n‚úÖ ZIP file created: {zip_filename}\")\n",
    "print(f\"üì¶ ZIP file size: {os.path.getsize(zip_filename) / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "# === FINAL CHECK ===\n",
    "print(\"\\nüìÅ Files in TFJS directory:\")\n",
    "for item in os.listdir(tfjs_output_dir):\n",
    "    print(\"  -\", item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i, met in enumerate(['accuracy', 'loss']): \n",
    "    ax[i].plot(history.history[met])\n",
    "    ax[i].plot(history.history['val_' + met])\n",
    "    ax[i].set_title(f'Model {met.capitalize()}')\n",
    "    ax[i].set_xlabel('Epochs')\n",
    "    ax[i].set_ylabel(met.capitalize())\n",
    "    ax[i].legend(['Train', 'Validation'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 524890,
     "sourceId": 963129,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 891819,
     "sourceId": 2332307,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1592399,
     "sourceId": 2619910,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2878166,
     "sourceId": 4962811,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 17810,
     "sourceId": 23812,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 29869,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
